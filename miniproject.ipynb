{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Project D7041E , Group 31\n",
    "\n",
    "## Jacob MÃ¶ller,  jacmll-9@student.ltu.se\n",
    "## Emil Wiklund, emiwik-9@student.ltu.se"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This mini project were done to futher improve our skills in and understanding of using MLP (multi-layer-percepton neural network) for supervised learning. The project utalizes the familiar dataset MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loadning image data from MNIST of handwritten digits.\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "#loads 60k training images and 10k testing images (pre setting of this data load method) \n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset  = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader  = DataLoader(dataset=test_dataset, batch_size=512, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source:\n",
    "\n",
    "https://www.datascienceweekly.org/tutorials/pytorch-mnist-load-mnist-dataset-from-pytorch-torchvision#:~:text=The%20MNIST%20dataset%20is%20comprised,28%20pixels%20by%2028%20pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One random data sample from the MNIST data set.\n"
     ]
    }
   ],
   "source": [
    "sample_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=True)\n",
    "images, labels = next(iter(sample_loader))\n",
    "\n",
    "image = images.squeeze().cpu().numpy()\n",
    "label= labels.item()\n",
    "\n",
    "plt.imshow(image, cmap='gray', extent=[0,28,28,0])\n",
    "plt.title(f\"Label: {label}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(\"One random data sample from the MNIST data set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, num_epochs=5):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            all_predictions.extend(predictions.numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    return accuracy\n",
    "\n",
    "#trying to get the confusion to work\n",
    "def test_model_2(model):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_images = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            all_predictions.extend(predictions.numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_images.extend(images)\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLPModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU() \n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1) #flatten the input\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)    \n",
    "        x = F.softmax(x, dim = 1) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the model above we use the *rectified Linear Unit* activations function since it does not suffer from the vanishing gradient problem. Which can be a pitfall for other activation functions such as the *sigmoid* or *hyperbolic tangent* activation function.\n",
    "\n",
    "source:\n",
    "\n",
    "https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 974    0    0    1    0    2    0    1    2    0]\n",
      " [   0 1120    4    2    0    2    1    2    4    0]\n",
      " [   6    2  989   18    2    2    5    5    3    0]\n",
      " [   1    0    3  983    0    9    0    9    5    0]\n",
      " [   4    0    4    0  942    2    7    4    3   16]\n",
      " [   7    0    1   13    0  859    4    2    5    1]\n",
      " [   9    3    3    1    1   12  923    2    4    0]\n",
      " [   2    6   17    6    0    2    0  991    0    4]\n",
      " [   4    0    3   15    4    6    4    5  932    1]\n",
      " [   6    6    2   16   18   16    1    7    4  933]]\n",
      "Accuracy on the test set: 0.9646\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Define parameters\n",
    "input_size = 28 * 28 # Is determined by the size of an image in MNIST\n",
    "hidden_size = 128\n",
    "output_size = 10\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "# start the model\n",
    "model = MLPModel(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train model\n",
    "train_model(model, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Test model\n",
    "#accuracy = test_model(model)\n",
    "\n",
    "# Test model with confusion\n",
    "accuracy = test_model_2(model)\n",
    "print(f\"Accuracy on the test set: {accuracy}\")\n",
    "\n",
    "# control how many times to iterate for parameters in cells below\n",
    "iterations_hidden = 6\n",
    "iterations_learnRate = 6\n",
    "iterations_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the confusion-matrix above the amount of correct prediction for a certain class is illustrated through the positional indexes.\n",
    "\n",
    "\n",
    "In the first row (row = 0) the class corresponds to the handwritten digit of the number 0, the first column we can see that 974 samples were correctly predicted compared with its label.\n",
    "\n",
    "\n",
    "At index (i = 0 , j = 3) we can see that a sample with label 0 was classified as the number 3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tune amount of hidden layers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While hidden_size changes:\n",
      "Iteration: 1 Epoch: 10 Learning rate: 0.001 Hidden size: 256\n",
      "Confusion Matrix:\n",
      "[[ 966    0    1    0    2    1    1    6    3    0]\n",
      " [   0 1125    2    3    0    1    1    1    2    0]\n",
      " [   8    3  978   15    1    0    1   12   12    2]\n",
      " [   0    1    1  987    0    5    0    9    3    4]\n",
      " [   2    0    4    0  943    1    3    9    0   20]\n",
      " [   5    1    1   12    3  857    3    3    3    4]\n",
      " [   6    4    1    1   11   14  914    2    5    0]\n",
      " [   0    4    8   10    0    0    0 1001    0    5]\n",
      " [   4    3    2   33    5    4    2    8  902   11]\n",
      " [   3    7    0   10    9    2    1   16    0  961]]\n",
      "Accuracy on the test set: 0.9634\n",
      "Iteration: 2 Epoch: 10 Learning rate: 0.001 Hidden size: 512\n",
      "Confusion Matrix:\n",
      "[[ 964    0    1    3    1    6    1    0    2    2]\n",
      " [   0 1122    3    3    0    2    2    1    2    0]\n",
      " [   6    1  983    7    6    0    3    6   18    2]\n",
      " [   0    0    5  972    1   15    0    3    6    8]\n",
      " [   0    0    3    0  964    1    2    1    1   10]\n",
      " [   2    0    1    3    2  876    3    0    1    4]\n",
      " [   9    3    1    1    9   14  916    1    4    0]\n",
      " [   0    8    8    6    4    1    1  947    0   53]\n",
      " [   1    1    2    2    6    7    1    3  940   11]\n",
      " [   3    2    0    3    9    3    1    1    1  986]]\n",
      "Accuracy on the test set: 0.967\n",
      "Iteration: 3 Epoch: 10 Learning rate: 0.001 Hidden size: 1024\n",
      "Confusion Matrix:\n",
      "[[ 971    0    1    0    0    0    5    1    2    0]\n",
      " [   0 1126    3    2    0    0    2    0    2    0]\n",
      " [  10    1  999    1    3    0    3    9    6    0]\n",
      " [   0    1    7  979    0    2    0    9   12    0]\n",
      " [   1    0    1    0  965    0    8    2    2    3]\n",
      " [   5    1    0   10    1  849   11    3   11    1]\n",
      " [   5    3    0    0    1    2  943    1    3    0]\n",
      " [   2    9   10    1    0    0    1 1002    3    0]\n",
      " [   5    0    2    4    4    0    4    6  948    1]\n",
      " [   3    9    1    7   26    3    1   20   11  928]]\n",
      "Accuracy on the test set: 0.971\n",
      "Iteration: 4 Epoch: 10 Learning rate: 0.001 Hidden size: 2048\n",
      "Confusion Matrix:\n",
      "[[ 965    1    5    1    0    0    5    1    2    0]\n",
      " [   0 1118    9    2    0    1    5    0    0    0]\n",
      " [   1    0 1015    6    1    0    2    6    1    0]\n",
      " [   0    0   12  985    0    4    0    4    4    1]\n",
      " [   1    2    9    0  945    1    8    3    1   12]\n",
      " [   5    0    1   18    1  849    7    1    6    4]\n",
      " [   6    2    4    0    1    2  939    1    3    0]\n",
      " [   0    7   26    1    1    0    1  988    0    4]\n",
      " [   2    0   23    6    2    3    4    2  929    3]\n",
      " [   3    8    5   12   10    2    2    6    1  960]]\n",
      "Accuracy on the test set: 0.9693\n",
      "Iteration: 5 Epoch: 10 Learning rate: 0.001 Hidden size: 4096\n",
      "Confusion Matrix:\n",
      "[[ 963    1    3    0    0    3    8    1    1    0]\n",
      " [   0 1119    8    3    1    1    3    0    0    0]\n",
      " [   5    0 1007    4    4    0    7    4    1    0]\n",
      " [   0    0   12  984    0    1    2    5    3    3]\n",
      " [   1    0    0    0  954    2   12    2    0   11]\n",
      " [   2    0    1   16    0  858    7    1    0    7]\n",
      " [   5    2    4    0    1    8  938    0    0    0]\n",
      " [   1    6   34    5    2    0    1  959    0   20]\n",
      " [   7    1   35   46    9   14    5    4  843   10]\n",
      " [   3    3    1   12   26    3    1    4    1  955]]\n",
      "Accuracy on the test set: 0.958\n",
      "Best hidden_size found: 1024 with accuracy: 0.971\n"
     ]
    }
   ],
   "source": [
    "best_hidden_size = 0\n",
    "best_hidden_size_accuracy = 0\n",
    "best_model = 0\n",
    "\n",
    "print(\"While hidden_size changes:\")\n",
    "for iteration in range(1, iterations_hidden):\n",
    "\n",
    "    hidden_size = hidden_size * 2 # x^2 increase \n",
    "    print(f\"Iteration: {iteration} Epoch: {num_epochs} Learning rate: {learning_rate} Hidden size: {hidden_size}\")\n",
    "\n",
    "    #need to instaciate a new model for every hidden layer_size\n",
    "    model = MLPModel(input_size, hidden_size, output_size)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_model(model, criterion, optimizer, num_epochs)\n",
    "\n",
    "    accuracy = test_model_2(model)\n",
    "    print(f\"Accuracy on the test set: {accuracy}\")\n",
    "\n",
    "    if accuracy > best_hidden_size_accuracy:\n",
    "        best_hidden_size_accuracy = accuracy\n",
    "        best_hidden_size = hidden_size\n",
    "\n",
    "print(f\"Best hidden_size found: {best_hidden_size} with accuracy: {best_hidden_size_accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tune learningrate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate = 0.001\n",
      "While learning_rate changes:\n",
      "Iteration: 1 Epoch: 10 Learning rate: 0.0001 Hidden size: 1024\n",
      "Confusion Matrix:\n",
      "[[ 968    0    1    3    0    3    1    2    2    0]\n",
      " [   0 1118    4    3    0    1    3    3    3    0]\n",
      " [   7    1  989    6    5    0    3   13    7    1]\n",
      " [   1    0    4  987    0    2    0    9    4    3]\n",
      " [   2    1    5    0  943    0    7    4    3   17]\n",
      " [   9    1    1   22    1  834    8    5    6    5]\n",
      " [  11    3    3    2    4    7  920    2    6    0]\n",
      " [   0    7   12    7    0    1    0  997    1    3]\n",
      " [   4    0    2   11    3    3    5   13  931    2]\n",
      " [   6    6    1   17   13    1    1   14    3  947]]\n",
      "Accuracy on the test set: 0.9634\n",
      "Iteration: 2 Epoch: 10 Learning rate: 1e-05 Hidden size: 1024\n",
      "Confusion Matrix:\n",
      "[[ 966    0    1    2    0    3    5    1    2    0]\n",
      " [   0 1122    3    1    1    1    4    1    2    0]\n",
      " [   6    2  994    5    5    0    3    8    8    1]\n",
      " [   0    0    6  981    0    4    1    7    8    3]\n",
      " [   1    0    3    0  954    0    7    1    2   14]\n",
      " [   7    1    1    7    1  854    8    3    6    4]\n",
      " [   6    2    0    1    4    7  932    1    5    0]\n",
      " [   0    9   16    2    3    1    0  982    2   13]\n",
      " [   3    0    3    6    3    3    5    4  944    3]\n",
      " [   6    6    1   13   12    1    1    8    4  957]]\n",
      "Accuracy on the test set: 0.9686\n",
      "Iteration: 3 Epoch: 10 Learning rate: 1.0000000000000002e-06 Hidden size: 1024\n",
      "Confusion Matrix:\n",
      "[[ 966    0    1    2    0    3    5    1    2    0]\n",
      " [   0 1121    3    1    0    1    4    2    3    0]\n",
      " [   6    2  996    5    5    0    3    8    6    1]\n",
      " [   0    0    6  982    0    4    1    7    7    3]\n",
      " [   1    0    4    0  952    0    7    1    2   15]\n",
      " [   6    1    1    7    1  858    7    2    5    4]\n",
      " [   6    2    0    1    3    7  933    1    5    0]\n",
      " [   0    7   17    2    2    1    0  986    2   11]\n",
      " [   3    0    3    7    3    4    5    5  941    3]\n",
      " [   6    6    1   13   11    2    1    8    4  957]]\n",
      "Accuracy on the test set: 0.9692\n",
      "Iteration: 4 Epoch: 10 Learning rate: 1.0000000000000002e-07 Hidden size: 1024\n",
      "Confusion Matrix:\n",
      "[[ 966    0    1    2    0    3    5    1    2    0]\n",
      " [   0 1121    3    1    0    1    4    2    3    0]\n",
      " [   6    2  995    5    5    0    3    8    7    1]\n",
      " [   0    0    6  982    0    4    1    7    7    3]\n",
      " [   1    0    4    0  952    0    7    1    2   15]\n",
      " [   6    1    1    7    1  857    8    2    5    4]\n",
      " [   6    2    0    1    3    7  933    1    5    0]\n",
      " [   0    7   17    2    2    1    0  986    2   11]\n",
      " [   3    0    3    7    3    4    5    5  941    3]\n",
      " [   6    6    1   13   11    2    1    8    4  957]]\n",
      "Accuracy on the test set: 0.969\n",
      "Iteration: 5 Epoch: 10 Learning rate: 1.0000000000000002e-08 Hidden size: 1024\n",
      "Confusion Matrix:\n",
      "[[ 966    0    1    2    0    3    5    1    2    0]\n",
      " [   0 1121    3    1    0    1    4    2    3    0]\n",
      " [   6    2  995    5    5    0    3    8    7    1]\n",
      " [   0    0    6  982    0    4    1    7    7    3]\n",
      " [   1    0    4    0  952    0    7    1    2   15]\n",
      " [   6    1    1    7    1  857    8    2    5    4]\n",
      " [   6    2    0    1    3    7  933    1    5    0]\n",
      " [   0    7   17    2    2    1    0  986    2   11]\n",
      " [   3    0    3    7    3    4    5    5  941    3]\n",
      " [   6    6    1   13   11    2    1    8    4  957]]\n",
      "Accuracy on the test set: 0.969\n",
      "Best learning rate found: 1.0000000000000002e-06 with accuracy: 0.9692\n"
     ]
    }
   ],
   "source": [
    "best_learning_rate = 0\n",
    "best_accuracy_learnRate = 0\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "#init a model with the best hidden_size\n",
    "model = MLPModel(input_size, best_hidden_size, output_size)\n",
    "\n",
    "print(f\"learning rate = {learning_rate}\")\n",
    "\n",
    "\n",
    "print(\"While learning_rate changes:\")\n",
    "for iteration in range(1, iterations_learnRate):\n",
    "    learning_rate = learning_rate / 10\n",
    "    print(f\"Iteration: {iteration} Epoch: {num_epochs} Learning rate: {learning_rate} Hidden size: {best_hidden_size}\")\n",
    "\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_model(model, criterion, optimizer, num_epochs)\n",
    "\n",
    "    accuracy = test_model_2(model)\n",
    "    print(f\"Accuracy on the test set: {accuracy}\")\n",
    "\n",
    "    if accuracy > best_accuracy_learnRate:\n",
    "        best_accuracy_learnRate = accuracy\n",
    "        best_learning_rate = learning_rate\n",
    "\n",
    "print(f\"Best learning rate found: {best_learning_rate} with accuracy: {best_accuracy_learnRate}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tune number of epochs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While num_epochs changes:\n",
      "Iteration: 1 Epoch: 30 Learning rate: 1.0000000000000002e-06 Hidden size: 1024\n",
      "Confusion Matrix:\n",
      "[[ 966    0    1    2    0    3    5    1    2    0]\n",
      " [   0 1121    3    1    0    1    4    2    3    0]\n",
      " [   6    2  997    5    4    0    4    8    5    1]\n",
      " [   0    0    6  982    0    4    1    7    7    3]\n",
      " [   1    0    5    0  952    0    6    1    2   15]\n",
      " [   6    1    1    7    1  858    7    2    5    4]\n",
      " [   6    2    0    1    3    7  933    1    5    0]\n",
      " [   0    7   16    2    2    1    0  987    2   11]\n",
      " [   3    0    3    7    3    4    5    5  941    3]\n",
      " [   5    6    1   13   11    2    1   10    4  956]]\n",
      "Accuracy on the test set: 0.9693\n",
      "Iteration: 2 Epoch: 40 Learning rate: 1.0000000000000002e-06 Hidden size: 1024\n",
      "Confusion Matrix:\n",
      "[[ 966    0    1    2    0    3    5    1    2    0]\n",
      " [   0 1122    3    1    0    1    4    1    3    0]\n",
      " [   6    2  998    5    3    0    4    8    5    1]\n",
      " [   0    0    6  981    0    3    1    7    7    5]\n",
      " [   1    0    5    0  954    0    6    1    2   13]\n",
      " [   5    1    1    7    1  859    7    2    5    4]\n",
      " [   6    2    0    1    3    7  933    1    5    0]\n",
      " [   0    7   16    1    1    1    0  989    3   10]\n",
      " [   3    0    3    6    3    4    5    5  942    3]\n",
      " [   4    6    1   13   11    2    1    9    4  958]]\n",
      "Accuracy on the test set: 0.9702\n",
      "Iteration: 3 Epoch: 50 Learning rate: 1.0000000000000002e-06 Hidden size: 1024\n",
      "Confusion Matrix:\n",
      "[[ 968    0    1    2    0    3    3    1    2    0]\n",
      " [   0 1123    3    1    0    1    4    1    2    0]\n",
      " [   5    2 1001    5    4    0    2    8    5    0]\n",
      " [   0    0    6  983    0    3    1    7    6    4]\n",
      " [   1    0    4    0  956    0    6    1    2   12]\n",
      " [   5    1    1    7    1  859    7    2    5    4]\n",
      " [   6    2    0    1    2    7  934    1    5    0]\n",
      " [   0    7   16    1    1    1    0  990    3    9]\n",
      " [   3    0    3    6    3    4    5    5  942    3]\n",
      " [   4    6    1   13   10    2    1    9    4  959]]\n",
      "Accuracy on the test set: 0.9715\n",
      "Iteration: 4 Epoch: 60 Learning rate: 1.0000000000000002e-06 Hidden size: 1024\n",
      "Confusion Matrix:\n",
      "[[ 968    0    1    2    0    3    3    1    2    0]\n",
      " [   0 1123    3    1    0    1    4    1    2    0]\n",
      " [   5    2 1002    5    3    0    2    8    5    0]\n",
      " [   0    0    4  985    0    3    1    7    6    4]\n",
      " [   1    0    4    0  957    0    5    1    2   12]\n",
      " [   5    1    1    7    1  859    7    2    5    4]\n",
      " [   6    2    0    1    2    7  934    1    5    0]\n",
      " [   0    7   15    1    1    1    0  990    3   10]\n",
      " [   3    0    3    5    3    3    5    5  944    3]\n",
      " [   4    6    1   13   10    2    1    9    4  959]]\n",
      "Accuracy on the test set: 0.9721\n",
      "Iteration: 5 Epoch: 70 Learning rate: 1.0000000000000002e-06 Hidden size: 1024\n",
      "Confusion Matrix:\n",
      "[[ 968    0    1    2    0    3    3    1    2    0]\n",
      " [   0 1122    3    1    0    1    4    1    3    0]\n",
      " [   4    2 1003    5    4    0    1    7    6    0]\n",
      " [   0    0    5  985    0    3    1    7    5    4]\n",
      " [   1    0    4    0  956    0    5    1    2   13]\n",
      " [   5    1    1    6    1  861    7    1    5    4]\n",
      " [   6    2    0    1    1    7  933    2    6    0]\n",
      " [   0    8   14    1    1    1    0  988    4   11]\n",
      " [   3    0    3    5    3    3    5    5  944    3]\n",
      " [   4    6    1   13   10    3    1    8    2  961]]\n",
      "Accuracy on the test set: 0.9721\n",
      "Iteration: 6 Epoch: 80 Learning rate: 1.0000000000000002e-06 Hidden size: 1024\n",
      "Confusion Matrix:\n",
      "[[ 968    0    1    2    0    3    3    1    2    0]\n",
      " [   0 1123    3    1    0    1    4    1    2    0]\n",
      " [   4    1 1004    5    4    0    1    7    6    0]\n",
      " [   0    0    5  986    0    3    1    7    4    4]\n",
      " [   1    0    4    0  956    0    5    1    2   13]\n",
      " [   5    0    1    6    1  864    6    1    5    3]\n",
      " [   6    2    0    1    1    7  933    2    6    0]\n",
      " [   0    8   13    1    1    1    0  991    3   10]\n",
      " [   3    0    3    5    3    2    4    5  946    3]\n",
      " [   4    5    1   13    9    2    1    9    2  963]]\n",
      "Accuracy on the test set: 0.9734\n",
      "Iteration: 7 Epoch: 90 Learning rate: 1.0000000000000002e-06 Hidden size: 1024\n",
      "Confusion Matrix:\n",
      "[[ 968    0    1    2    0    2    2    2    3    0]\n",
      " [   0 1122    3    1    0    1    4    2    2    0]\n",
      " [   4    1 1004    5    3    0    1    8    6    0]\n",
      " [   0    0    5  987    0    3    1    7    3    4]\n",
      " [   1    0    4    0  956    0    5    1    2   13]\n",
      " [   5    0    1    6    1  864    6    1    5    3]\n",
      " [   6    2    0    1    1    6  934    2    6    0]\n",
      " [   0    7   12    1    0    1    0  994    4    9]\n",
      " [   3    0    3    5    3    2    4    5  947    2]\n",
      " [   4    5    1   13   10    2    1    9    2  962]]\n",
      "Accuracy on the test set: 0.9738\n",
      "Iteration: 8 Epoch: 100 Learning rate: 1.0000000000000002e-06 Hidden size: 1024\n",
      "Confusion Matrix:\n",
      "[[ 969    0    1    2    0    2    2    2    2    0]\n",
      " [   0 1122    3    1    0    1    4    2    2    0]\n",
      " [   4    1 1005    5    3    0    1    8    5    0]\n",
      " [   0    0    4  988    0    3    1    7    4    3]\n",
      " [   1    0    3    0  956    0    5    2    2   13]\n",
      " [   5    0    0    6    1  865    6    1    5    3]\n",
      " [   6    2    0    1    1    6  934    2    6    0]\n",
      " [   0    9   11    1    0    1    0  997    2    7]\n",
      " [   4    0    3    3    3    2    4    6  946    3]\n",
      " [   4    4    1   12   10    2    1    9    2  964]]\n",
      "Accuracy on the test set: 0.9746\n",
      "Iteration: 9 Epoch: 110 Learning rate: 1.0000000000000002e-06 Hidden size: 1024\n",
      "Confusion Matrix:\n",
      "[[ 970    0    1    2    0    2    2    1    2    0]\n",
      " [   0 1121    3    1    0    1    4    2    3    0]\n",
      " [   4    1 1004    5    3    0    1    8    6    0]\n",
      " [   0    0    5  988    0    2    1    7    4    3]\n",
      " [   1    0    3    0  958    0    4    2    2   12]\n",
      " [   4    0    0    6    1  866    6    1    5    3]\n",
      " [   6    2    0    1    1    6  935    2    5    0]\n",
      " [   0    7   11    1    0    1    0  997    3    8]\n",
      " [   4    0    3    3    3    2    4    6  946    3]\n",
      " [   4    4    1   11   10    2    1    7    2  967]]\n",
      "Accuracy on the test set: 0.9752\n",
      "Best number of epochs found: 110 with accuracy: 0.9752\n"
     ]
    }
   ],
   "source": [
    "best_num_epochs = 0\n",
    "best_num_epoch_accuracy = 0\n",
    "\n",
    "\n",
    "print(\"While num_epochs changes:\")\n",
    "for iteration in range(1, iterations_epochs):\n",
    "    num_epochs += 10\n",
    "    print(f\"Iteration: {iteration} Epoch: {num_epochs} Learning rate: {best_learning_rate} Hidden size: {best_hidden_size}\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_learning_rate)\n",
    "\n",
    "    train_model(model, criterion, optimizer, num_epochs)\n",
    "\n",
    "    accuracy = test_model_2(model)\n",
    "    print(f\"Accuracy on the test set: {accuracy}\")\n",
    "\n",
    "    if accuracy > best_num_epoch_accuracy:\n",
    "        best_num_epoch_accuracy = accuracy\n",
    "        best_num_epochs = num_epochs\n",
    "\n",
    "print(f\"Best number of epochs found: {best_num_epochs} with accuracy: {best_num_epoch_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings\n",
    "\n",
    "\n",
    "1. While tuning the amount of hidden layers within the model we found that the accuracy gains diminishes after layers size 1024 (2^10). This is probably due to the model becoming too complex for the classification and thus overfitting occurs so that the model won't be generalized to patterns in the data, instead becoming a bit more biased to the training data.\n",
    "\n",
    "\n",
    "2. We expected the learning rate to have the most impact of the parameters to be tuned but we had some strange results, while decreasing the learning from our initial value 0.001 by a factor of 0.1 the accuracy was negatively affected. We then did some further testing with the initial learning rate since we assumed that we stepped past the minimum lost function already in the first iteration of our loop to tune the learning rate. Sadly this assumtion were not accurate see the last markdown section of the notebook.\n",
    "\n",
    "\n",
    "3. As expected, increasing the number of epochs slightly increased the accuracy for each iteration, naturally at the price of an increase in execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters found: hidden layers = 1024, learning rate = 1.0000000000000002e-06, number of epochs = 110\n",
      "some manual testing for learning rate = 0.001 \n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 973    1    1    0    1    0    0    3    1    0]\n",
      " [   0 1127    5    1    0    0    0    1    1    0]\n",
      " [   3    1 1015    1    1    0    3    8    0    0]\n",
      " [   3    0    7  990    0    0    0    7    2    1]\n",
      " [   2    1    6    0  951    0    3    9    0   10]\n",
      " [   3    2    0   18    0  847    5   12    3    2]\n",
      " [   7    4    1    0    3    2  938    3    0    0]\n",
      " [   0    4    9    1    0    0    0 1013    0    1]\n",
      " [  11    0    8    1    2    2    3    6  936    5]\n",
      " [   2    2    2    4   16    0    1   21    2  959]]\n",
      "Accuracy on the test set: 0.9749\n"
     ]
    }
   ],
   "source": [
    "#best parameters found: hidden layers = 1024, learning rate = 1.0000000000000002e-06, number of epochs = 110  | 24-01-09 17:40\n",
    "\n",
    "print(f\"best parameters found: hidden layers = {best_hidden_size}, learning rate = {best_learning_rate}, number of epochs = {best_num_epochs}\")\n",
    "\n",
    "#trying with the best learning from the first iterations of tests when we tuned the hidden_size parameter, where the best learning rate seemed to be 0.001,\n",
    "# since the accuracy were lower with the best hidden_size and learning_rate = 0.0001 in the first iteration while changening the learning rate.\n",
    "\n",
    "print(\"some manual testing for learning rate = 0.001 \\n\")\n",
    "# start the model with the best parameters\n",
    "model = MLPModel(input_size, best_hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train model with the best parameters \n",
    "train_model(model, criterion, optimizer, best_num_epochs)\n",
    "\n",
    "# Test model with confusion\n",
    "accuracy = test_model_2(model)\n",
    "print(f\"Accuracy on the test set: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "*The best parametes found:*\n",
    "```best parameters found: hidden layers = 1024, learning rate = 1.0000000000000002e-06, number of epochs = 110```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
